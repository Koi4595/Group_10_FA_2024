{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b90adb-1831-4354-8e4d-38b30dc7e7ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm  \n\u001b[0;32m      9\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject_data/cleaned_job_company_pair.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\sentence_transformers\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Literal, overload\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, nn\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\__init__.py:1187\u001b[0m\n\u001b[0;32m   1180\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m \n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[1;32m-> 1187\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _StorageBase\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\functional.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopt_einsum\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mopt_einsum\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lowrank\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m svd_lowrank, pca_lowrank\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n\u001b[0;32m     12\u001b[0m     handle_torch_function)\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\nn\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[0;32m      4\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[0;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\nn\\parallel\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_parallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel, data_parallel\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscatter_gather\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scatter, gather\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedDataParallel\n\u001b[0;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparallel_apply\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgather\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_parallel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataParallel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistributedDataParallel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mDistributedDataParallelCPU\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\nn\\parallel\\distributed.py:256\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m        Syncs the final model to ensure that the model is the same across all\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m        processes.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mddp\u001b[38;5;241m.\u001b[39m_sync_final_model(is_last_joiner)\n\u001b[1;32m--> 256\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDistributedDataParallel\u001b[39;00m(Module, Joinable):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Implements distributed data parallelism that is based on\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    ``torch.distributed`` package at the module level.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m        >>> net = torch.nn.parallel.DistributedDataParallel(model)\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;66;03m# used to track whether the given thread is inside ddp forward for torchdynamo purposes\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\nn\\parallel\\distributed.py:1453\u001b[0m, in \u001b[0;36mDistributedDataParallel\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin_device\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m   1452\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@property\u001b[39;49m\n\u001b[1;32m-> 1453\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mjoin_process_group\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_group\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_register_buffer_comm_hook\u001b[39m(\n\u001b[0;32m   1457\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1458\u001b[0m     state,\n\u001b[0;32m   1459\u001b[0m     hook: Callable,\n\u001b[0;32m   1460\u001b[0m     comm_hook_location\u001b[38;5;241m=\u001b[39m_BufferCommHookLocation\u001b[38;5;241m.\u001b[39mPOST_FORWARD,\n\u001b[0;32m   1461\u001b[0m ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm  \n",
    "\n",
    "\n",
    "data_path = \"project_data/cleaned_job_company_pair.csv\"  \n",
    "raw_data = pd.read_csv(data_path)\n",
    "if 'description' not in raw_data.columns:\n",
    "    raw_data['description'] = raw_data['skill_name'] + \" in \" + raw_data['industry_name']\n",
    "\n",
    "print(\"Loading lightweight embedding model...\")\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')  \n",
    "print(\"Model loaded.\")\n",
    "\n",
    "\n",
    "print(\"Normalizing numerical features...\")\n",
    "\n",
    "numerical_features = ['annual_min_salary', 'annual_max_salary', 'employee_count', 'follower_count']\n",
    "scaler = StandardScaler()\n",
    "raw_data[numerical_features] = scaler.fit_transform(raw_data[numerical_features].fillna(0))\n",
    "\n",
    "\n",
    "text_features = ['description']\n",
    "text_embeddings = []\n",
    "\n",
    "print(\"Embedding text features...\")\n",
    "for feature in tqdm(text_features, desc=\"Text Embeddings\"):\n",
    "    raw_data[feature] = raw_data[feature].fillna('')\n",
    "    embeddings = model.encode(raw_data[feature].tolist(), batch_size=128, show_progress_bar=True)\n",
    "    text_embeddings.append(embeddings)\n",
    "\n",
    "\n",
    "print(\"Combining text embeddings...\")\n",
    "combined_text_embeddings = np.hstack(text_embeddings)\n",
    "\n",
    "\n",
    "print(\"Merging features...\")\n",
    "combined_features = np.hstack([combined_text_embeddings, raw_data[numerical_features].values])\n",
    "\n",
    "\n",
    "def create_faiss_index(embeddings, nlist=100):\n",
    "    d = embeddings.shape[1]  \n",
    "    quantizer = faiss.IndexFlatL2(d)  \n",
    "    index = faiss.IndexIVFPQ(quantizer, d, nlist, 8, 8)  \n",
    "    print(\"Training FAISS index...\")\n",
    "    index.train(embeddings)  \n",
    "    print(\"Adding embeddings to FAISS index...\")\n",
    "    index.add(embeddings)  \n",
    "    print(\"FAISS index created.\")\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed329b0a-1b0b-447b-8e8c-d32137ac419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings, nlist=100, m=8):\n",
    "    d = embeddings.shape[1]  \n",
    "    \n",
    "    \n",
    "    if d % m != 0:\n",
    "        print(f\"Vector dimension {d} is not a multiple of {m}. Padding to the next multiple.\")\n",
    "        new_d = (d // m + 1) * m\n",
    "        embeddings = np.pad(embeddings, ((0, 0), (0, new_d - d)), mode='constant', constant_values=0)\n",
    "        d = new_d  \n",
    "    \n",
    "    quantizer = faiss.IndexFlatL2(d)  \n",
    "    index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)  \n",
    "    print(\"Training FAISS index...\")\n",
    "    index.train(embeddings)  \n",
    "    print(\"Adding embeddings to FAISS index...\")\n",
    "    index.add(embeddings)  \n",
    "    print(\"FAISS index created.\")\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Building FAISS index...\")\n",
    "faiss_index = create_faiss_index(np.array(combined_features))\n",
    "faiss.write_index(faiss_index, \"job_similarity_index.ivfpq\")\n",
    "np.save(\"job_features.npy\", combined_features)\n",
    "print(\"Index and features saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98738b-1a93-4fae-95a6-0237a7f04160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_jobs(user_description, numerical_inputs, top_n=25):\n",
    "    print(\"Encoding user query...\")\n",
    "    \n",
    "    user_embedding = model.encode([user_description], show_progress_bar=False)\n",
    "\n",
    "    \n",
    "    user_numerical = np.array([numerical_inputs[col] for col in numerical_features]).reshape(1, -1)\n",
    "    numerical_scaled = scaler.transform(user_numerical)  # 标准化\n",
    "\n",
    "    \n",
    "    query_features = np.hstack([user_embedding, numerical_scaled])\n",
    "    #print(len(query_features))\n",
    "    \n",
    "    if query_features.shape[1] != faiss_index.d:\n",
    "        print(f\"Adjusting query features from {query_features.shape[1]} to {faiss_index.d}.\")\n",
    "        if query_features.shape[1] < faiss_index.d:\n",
    "            \n",
    "            print(\"here------------------------------------------\")\n",
    "            padding = np.zeros((1, faiss_index.d - query_features.shape[1]))\n",
    "            query_features = np.hstack([query_features, padding])\n",
    "        else:\n",
    "            \n",
    "            query_features = query_features[:, :faiss_index.d]\n",
    "\n",
    "    print(\"Searching for similar jobs...\")\n",
    "    D, I = faiss_index.search(query_features.reshape(1, -1), top_n)\n",
    "    results = []\n",
    "\n",
    "    for idx in I[0]:\n",
    "        if idx < len(raw_data):\n",
    "            results.append(raw_data.iloc[idx])\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "queries = [\n",
    "    {\n",
    "        \"query\": \"I want to be an engineer. I have good python skills and knowledge about machine learning.\",\n",
    "        \"numerical_inputs\": {\n",
    "            'annual_min_salary': 0,\n",
    "            'annual_max_salary': 80000,\n",
    "            'employee_count': 1000,\n",
    "            'follower_count': 500,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Looking for a software developer position specializing in cloud computing.\",\n",
    "        \"numerical_inputs\": {\n",
    "            'annual_min_salary': 60000,\n",
    "            'annual_max_salary': 120000,\n",
    "            'employee_count': 200,\n",
    "            'follower_count': 300,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I have experience in data science and want a job in data analytics.\",\n",
    "        \"numerical_inputs\": {\n",
    "            'annual_min_salary': 50000,\n",
    "            'annual_max_salary': 90000,\n",
    "            'employee_count': 500,\n",
    "            'follower_count': 400,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I wanna be a teacher.\",\n",
    "        \"numerical_inputs\": {\n",
    "            'annual_min_salary': 0,\n",
    "            'annual_max_salary': 150000,\n",
    "            'employee_count': 1500,\n",
    "            'follower_count': 600,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Loop through each query and execute the query_jobs function\n",
    "for i, entry in enumerate(queries, start=1):\n",
    "    print(f\"\\nPerforming query {i}: {entry['query']}\")\n",
    "    result = query_jobs(entry[\"query\"], numerical_inputs=entry[\"numerical_inputs\"], top_n=10)\n",
    "    print(f\"Top matching jobs for query {i}:\")\n",
    "    print(result[['company_name', 'description','skill_name', 'industry_name']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1ef95-ea08-4fc2-9884-16060f20169d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
